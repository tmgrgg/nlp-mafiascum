# -*- coding: utf-8 -*-
"""MafiaBert consolidate nice max pool seed20 0.58 0.29 replicate

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E8Md3DRRWhIuMR16e7qNy5eUdQTQRFrK
"""

from google.colab import drive
drive.mount('/content/drive')

"""{
  "eval_ACC:": 0.5670103092783505,
  "eval_F1:": 0.3336980306345733,
  "eval_AP:": 0.2615190416146073,
  "eval_AUROC:": 0.5359966258681855,
  "eval_LOSS:": 0.6906432061080812,
  "learning_rate": 1.8984565393988624e-06,
  "loss": 0.7005807433128357,
  "step": 500
}{
  "eval_ACC:": 0.5776158312596279,
  "eval_F1:": 0.33242812997471677,
  "eval_AP:": 0.2636342095378782,
  "eval_AUROC:": 0.5412800505644543,
  "eval_LOSS:": 0.6848660885143484,
  "learning_rate": 1.7969130787977254e-06,
  "loss": 0.6935685929059983,
  "step": 1000
}{
  "eval_ACC:": 0.6403009835288541,
  "eval_F1:": 0.29529889727219966,
  "eval_AP:": 0.2685985657489121,
  "eval_AUROC:": 0.5506486459513983,
  "eval_LOSS:": 0.6596342944088162,
  "learning_rate": 1.6953696181965879e-06,
  "loss": 0.6891786968708038,
  "step": 1500
}
{
  "eval_ACC:": 0.5661808271122171,
  "eval_F1:": 0.35771929824561405,
  "eval_AP:": 0.27593838794061776,
  "eval_AUROC:": 0.5582315177177563,
  "eval_LOSS:": 0.6759917593027747,
  "learning_rate": 1.5938261575954508e-06,
  "loss": 0.6905368512868881,
  "step": 2000
}
{
  "eval_ACC:": 0.5929612513330963,
  "eval_F1:": 0.3464611872146119,
  "eval_AP:": 0.2789256684555318,
  "eval_AUROC:": 0.5606398209333989,
  "eval_LOSS:": 0.6661618873238606,
  "learning_rate": 1.4922826969943133e-06,
  "loss": 0.6912294577360153,
  "step": 2500
}
{
  "eval_ACC:": 0.5555753051309397,
  "eval_F1:": 0.3644836058629162,
  "eval_AP:": 0.2800485694015553,
  "eval_AUROC:": 0.5614936093468204,
  "eval_LOSS:": 0.6839857909694669,
  "learning_rate": 1.3907392363931763e-06,
  "loss": 0.6829688959121704,
  "step": 3000
}{
  "eval_ACC:": 0.5480507169095864,
  "eval_F1:": 0.37310979618671924,
  "eval_AP:": 0.284813161623198,
  "eval_AUROC:": 0.5667256236797521,
  "eval_LOSS:": 0.6876807217296936,
  "learning_rate": 1.2891957757920388e-06,
  "loss": 0.683927996635437,
  "step": 3500
}
{
  "eval_ACC:": 0.624837066003081,
  "eval_F1:": 0.33192656678624183,
  "eval_AP:": 0.28336253127027267,
  "eval_AUROC:": 0.566959294188652,
  "eval_LOSS:": 0.6524681791845042,
  "learning_rate": 1.1876523151909017e-06,
  "loss": 0.6848519400358201,
  "step": 4000
}
{
  "eval_ACC:": 0.5690247659675317,
  "eval_F1:": 0.37314719062392276,
  "eval_AP:": 0.28767472640294234,
  "eval_AUROC:": 0.5702023861289917,
  "eval_LOSS:": 0.6802309849889361,
  "learning_rate": 1.0861088545897643e-06,
  "loss": 0.6815990030765533,
  "step": 4500
}
{
  "eval_ACC:": 0.624837066003081,
  "eval_F1:": 0.33501365259399285,
  "eval_AP:": 0.2901803970075101,
  "eval_AUROC:": 0.57090539362099,
  "eval_LOSS:": 0.6482908137584978,
  "learning_rate": 9.84565393988627e-07,
  "loss": 0.6839097678661347,
  "step": 5000
}
{
  "eval_ACC:": 0.6278587510368527,
  "eval_F1:": 0.33428722840487546,
  "eval_AP:": 0.28932784173241705,
  "eval_AUROC:": 0.571039627039627,
  "eval_LOSS:": 0.6436712444490739,
  "learning_rate": 8.830219333874897e-07,
  "loss": 0.6724847514629364,
  "step": 5500
}
{
  "eval_ACC:": 0.5840146936840858,
  "eval_F1:": 0.36753445635528326,
  "eval_AP:": 0.2910243426237159,
  "eval_AUROC:": 0.5732756974591837,
  "eval_LOSS:": 0.6725532255889342,
  "learning_rate": 7.814784727863525e-07,
  "loss": 0.6755852544307709,
  "step": 6000
}
{
  "eval_ACC:": 0.6209858988031758,
  "eval_F1:": 0.3506243021013095,
  "eval_AP:": 0.2906003708187003,
  "eval_AUROC:": 0.574961309163144,
  "eval_LOSS:": 0.648794225594425,
  "learning_rate": 6.799350121852152e-07,
  "loss": 0.6752851024270058,
  "step": 6500
}
{
  "eval_ACC:": 0.631413674605996,
  "eval_F1:": 0.3430140458337733,
  "eval_AP:": 0.2921047627682009,
  "eval_AUROC:": 0.5758108205080683,
  "eval_LOSS:": 0.6417344681877126,
  "learning_rate": 5.783915515840779e-07,
  "loss": 0.6768719785809517,
  "step": 7000
}
{
  "eval_ACC:": 0.6561203934115416,
  "eval_F1:": 0.31524303916941954,
  "eval_AP:": 0.2914963558817698,
  "eval_AUROC:": 0.5743571874948021,
  "eval_LOSS:": 0.6258573317177446,
  "learning_rate": 4.768480909829406e-07,
  "loss": 0.6708497121334076,
  "step": 7500
}
{
  "eval_ACC:": 0.5731129280720465,
  "eval_F1:": 0.3726599912929909,
  "eval_AP:": 0.294914148494728,
  "eval_AUROC:": 0.5757341849451941,
  "eval_LOSS:": 0.6794291444194304,
  "learning_rate": 3.753046303818034e-07,
  "loss": 0.6672410234808922,
  "step": 8000
}
{
  "eval_ACC:": 0.5509539044910534,
  "eval_F1:": 0.38226424321460595,
  "eval_AP:": 0.29757222531792366,
  "eval_AUROC:": 0.5778351570278175,
  "eval_LOSS:": 0.6945631463623171,
  "learning_rate": 2.737611697806661e-07,
  "loss": 0.6655132434368134,
  "step": 8500
}
{
  "eval_ACC:": 0.5805190188410949,
  "eval_F1:": 0.3720063863757318,
  "eval_AP:": 0.295974680674165,
  "eval_AUROC:": 0.5765270964170047,
  "eval_LOSS:": 0.6754084418642887,
  "learning_rate": 1.7221770917952884e-07,
  "loss": 0.6642777700424194,
  "step": 9000
}
{
  "eval_ACC:": 0.5337125251807087,
  "eval_F1:": 0.3844830283122165,
  "eval_AP:": 0.2976467485574042,
  "eval_AUROC:": 0.5774512188457143,
  "eval_LOSS:": 0.7068453373248182,
  "learning_rate": 7.067424857839155e-08,
  "loss": 0.6680232243537902,
  "step": 9500
}
"""

# import pandas as pd
# import torch
# from torchtext import data
# from torchtext import datasets
# import random
# import numpy as np

# SEED = 9
# torch.backends.cudnn.deterministic = True
# np.random.seed(SEED)
# random.seed(SEED)
# torch.manual_seed(SEED)

# DOCS_PATH = 'drive/My Drive/MAFIA_DATA/24h_of_deception_basic.pkl'

# docs = pd.read_pickle(DOCS_PATH)
# docs = docs.sample(frac=1)
# docs.head()

# print(len(docs))

# # tokeniztion is done by BERT
# docs['content'] = docs['words'].apply(lambda x: ' '.join(x))

# TEST_RATIO = 0.5
# VALID_RATIO = 0.3

# # split at user level to stop leakage (plus, we want to be agnostic to author identity!)
# users = docs.author.unique()
# np.random.shuffle(users)

# train_users = users[int(len(users)*TEST_RATIO):]
# test_users = users[:int(len(users)*TEST_RATIO)]
# valid_users = train_users[:int(len(train_users)*VALID_RATIO)]
# train_users = train_users[int(len(train_users)*VALID_RATIO):]

# train_docs = docs[np.isin(docs['author'].values, train_users)]
# valid_docs = docs[np.isin(docs['author'].values, valid_users)]
# test_docs = docs[np.isin(docs['author'].values, test_users)]

# print(docs.iloc[0]['author'] == 'Egg')
# print(docs.iloc[1]['author'] == 'Wake88')
# print(train_docs.iloc[0]['author'] == 'Egg')
# print(train_docs.iloc[1]['author'] == 'Vendagoat')
# print(valid_docs.iloc[0]['author'] == 'Wake88')
# print(valid_docs.iloc[1]['author'] == 'OhGodMyLife')
# print(test_docs.iloc[0]['author'] == 'Bellaphant')
# print(test_docs.iloc[1]['author'] == 'Fishbulb')

import pandas as pd
import torch
from torchtext import data
from torchtext import datasets
import random
import numpy as np

# Set the seed
SEED = 42
torch.backends.cudnn.deterministic = True
np.random.seed(SEED)
random.seed(SEED)
torch.manual_seed(SEED)

# Download the pickle file from the Google Drive link and store it on Colab
# gdd.download_file_from_google_drive(file_id = '1-Aly8wEZrFAk4wr_rKH_JWfMvTBEJBoC',
#                                     dest_path = './24h_of_deception_basic.pkl',
#                                     unzip = False,
#                                     showsize = True)

# Read in the data from the pkl file and reset the indices
# in order to get rid of any indices that might be empty

DOCS_PATH = 'drive/My Drive/MAFIA_DATA/24h_of_deception_basic.pkl'

docs = pd.read_pickle(DOCS_PATH)
docs = docs.sample(frac=1)
docs

# Ratios for Validation and Test Datasets
TEST_RATIO = 0.5
VALID_RATIO = 0.3

# Split at user level to stop leakage (plus, we want 
# to be agnostic to author identity!)
doc_ids = list(docs.index)

np.random.shuffle(doc_ids)

train_doc_ids = doc_ids[int(len(doc_ids)*TEST_RATIO):]
test_doc_ids = doc_ids[:int(len(doc_ids)*TEST_RATIO)]
valid_doc_ids = train_doc_ids[:int(len(train_doc_ids)*VALID_RATIO)]
train_doc_ids = train_doc_ids[int(len(train_doc_ids)*VALID_RATIO):]

train_docs = docs[np.isin(docs.index.values, train_doc_ids)]
valid_docs = docs[np.isin(docs.index.values, valid_doc_ids)]
test_docs = docs[np.isin(docs.index.values, test_doc_ids)]

np.sum(train_docs.index.values)

!pip install transformers

"""# mafia"""

# !git clone https://bitbucket.org/bopjesvla/thesis.git

# !mv thesis/src/docs.pkl .

"""# MaxPool"""

from transformers import BertPreTrainedModel, BertModel
from torch import nn

from torch.nn import CrossEntropyLoss, MSELoss

class BertMaxPoolMafiaClassification(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size*2, self.config.num_labels)

        self.init_weights()
        self.freeze_pretrain(False)

    def freeze_pretrain(self, is_freeze):
        # Freeze bert
        for p in self.bert.parameters():
          p.requires_grad = not is_freeze

    def forward(self, tokens_in_batch):
        length_batch=[]
        input_ids_batch=[]
        attention_mask_batch=[]

        for feature in tokens_in_batch: 
          if len(feature.input_ids) > (256-64-2)*4:
            remaining=feature.input_ids[-(256-64-2)*4:]
          else:
            remaining=feature.input_ids
          while remaining:
            inputs = tokenizer.encode_plus(remaining, max_length=256,pad_to_max_length=True, return_overflowing_tokens=True, stride=64)

            input_ids_batch.append(inputs['input_ids'])
            attention_mask_batch.append(inputs['attention_mask'])

            remaining=inputs.get('overflowing_tokens')

          length_batch.append(len(input_ids_batch))

        outputs = self.bert(
            torch.tensor(input_ids_batch, dtype=torch.long).to(device),
            attention_mask=torch.tensor(attention_mask_batch, dtype=torch.long).to(device)
        )

        from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence, pad_sequence
        padded_outputs = pad_sequence([outputs[1][begin:end] for begin,end in zip([0]+length_batch,length_batch)])
        ###
        pooled_output,_=torch.max(padded_outputs,dim=0)
        pooled_output = torch.cat((
            pooled_output, 
            torch.mean(padded_outputs,dim=0)
        ), dim=1)
        ###

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here

########
        labels = torch.tensor([feature.label for feature in tokens_in_batch]).to(device)
########
        if labels is not None:
            # loss_fct = MSELoss()
            # loss = loss_fct(logits.view(-1), labels.view(-1))
            loss_fct = CrossEntropyLoss(weight=torch.tensor([0.25, 0.75]).to(device))
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            outputs = (loss,) + outputs

        return outputs  # (loss), logits, (hidden_states), (attentions)

from transformers import BertPreTrainedModel, BertModel
from torch import nn

from torch.nn import CrossEntropyLoss, MSELoss

class BertLstmMafiaClassification(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels

        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(2, self.config.num_labels)

        ###
        self.rnn = nn.LSTM(config.hidden_size, 
                           2, 
                           num_layers=1, 
                           bidirectional=False, 
                           dropout=0.25,
                           )
        ###

        self.init_weights()
        self.freeze_pretrain(False)

    def freeze_pretrain(self, is_freeze):
        # Freeze bert
        for p in self.bert.parameters():
          p.requires_grad = not is_freeze


    def forward(self, tokens_in_batch):
        length_batch=[]
        input_ids_batch=[]
        attention_mask_batch=[]

        for feature in tokens_in_batch: 
          if len(feature.input_ids) > (384-128-2)*6:
            remaining=feature.input_ids[-(384-128-2)*6:]
          else:
            remaining=feature.input_ids
          while remaining:
            inputs = tokenizer.encode_plus(remaining, max_length=384,pad_to_max_length=True, return_overflowing_tokens=True, stride=128)

            input_ids_batch.append(inputs['input_ids'])
            attention_mask_batch.append(inputs['attention_mask'])

            remaining=inputs.get('overflowing_tokens')

          length_batch.append(len(input_ids_batch))

        outputs = self.bert(
            torch.tensor(input_ids_batch, dtype=torch.long).to(device),
            attention_mask=torch.tensor(attention_mask_batch, dtype=torch.long).to(device)
        )

        ## LSTM ##
        from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence
        packed = pack_sequence([outputs[1][begin:end] for begin,end in zip([0]+length_batch,length_batch)], enforce_sorted=False)

        rnn_output,_ = self.rnn(packed)
        unpacked_output, padded_length = pad_packed_sequence(rnn_output)
        pooled_output = unpacked_output[padded_length-1, torch.arange(unpacked_output.shape[1])]
        ##########

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here

########
        labels = torch.tensor([feature.label for feature in tokens_in_batch]).to(device)
########
        if labels is not None:
            # loss_fct = MSELoss()
            # loss = loss_fct(logits.view(-1), labels.view(-1))
            loss_fct = CrossEntropyLoss(weight=torch.tensor([0.25, 0.75]).to(device))
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            outputs = (loss,) + outputs
        return outputs  # (loss), logits, (hidden_states), (attentions)

import torch
import dataclasses
@dataclasses.dataclass
class MafiaConfig:
  model_type="bert"
  model_name_or_path="DeepPavlov/bert-base-cased-conversational"
  adam_epsilon=1e-08
  cache_dir=''
  config_name=''
  data_dir='/content'
  do_eval=True
  do_lower_case=True
  do_train=True
  eval_all_checkpoints=False
  evaluate_during_training=True
  fp16=False
  fp16_opt_level='O1'
  gradient_accumulation_steps=1
  learning_rate=2e-06
  local_rank=-1
  logging_steps=500
  max_grad_norm=1.0
  max_seq_length=384
  max_steps=-1
  n_gpu=0
  no_cuda=False
  num_train_epochs=4.0
  output_dir='/tmp/MRPC/'
  overwrite_cache=False
  overwrite_output_dir=False
  per_gpu_eval_batch_size=8
  per_gpu_train_batch_size=8
  save_steps=500
  seed=42
  server_ip=''
  server_port=''
  task_name='MRPC'
  tokenizer_name=''
  warmup_steps=0
  weight_decay=0.0

args = MafiaConfig()
device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()
args.device = device

ml_logger = []

import glob
import json
import logging
import os
import random

import numpy as np
import torch
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset
from torch.utils.data.distributed import DistributedSampler
from tqdm import tqdm, trange

from transformers import (
    WEIGHTS_NAME,
    AdamW,
    BertConfig,
    BertForSequenceClassification,
    BertTokenizer,
    get_linear_schedule_with_warmup
)

try:
    from torch.utils.tensorboard import SummaryWriter
except ImportError:
    from tensorboardX import SummaryWriter


logger = logging.getLogger(__name__)

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

def train(args, train_dataset, model, tokenizer):
    """ Train the model """
    if args.local_rank in [-1, 0]:
        tb_writer = SummaryWriter()

    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)
    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)
    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=8, collate_fn=lambda x:x)

    if args.max_steps > 0:
        t_total = args.max_steps
        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1
    else:
        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs

    # Prepare optimizer and schedule (linear warmup and decay)
    no_decay = ["bias", "LayerNorm.weight"]
    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
            "weight_decay": args.weight_decay,
        },
        {"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], "weight_decay": 0.0},
    ]

    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total
    )

    # Check if saved optimizer or scheduler states exist
    if os.path.isfile(os.path.join(args.model_name_or_path, "optimizer.pt")) and os.path.isfile(
        os.path.join(args.model_name_or_path, "scheduler.pt")
    ):
        # Load in optimizer and scheduler states
        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, "optimizer.pt")))
        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, "scheduler.pt")))

    if args.fp16:
        try:
            from apex import amp
        except ImportError:
            raise ImportError("Please install apex from https://www.github.com/nvidia/apex to use fp16 training.")
        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)

    # multi-gpu training (should be after apex fp16 initialization)
    if args.n_gpu > 1:
        model = torch.nn.DataParallel(model)

    # Distributed training (should be after apex fp16 initialization)
    if args.local_rank != -1:
        model = torch.nn.parallel.DistributedDataParallel(
            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True,
        )

    # Train!
    logger.info("***** Running training *****")
    logger.info("  Num examples = %d", len(train_dataset))
    logger.info("  Num Epochs = %d", args.num_train_epochs)
    logger.info("  Instantaneous batch size per GPU = %d", args.per_gpu_train_batch_size)
    logger.info(
        "  Total train batch size (w. parallel, distributed & accumulation) = %d",
        args.train_batch_size
        * args.gradient_accumulation_steps
        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),
    )
    logger.info("  Gradient Accumulation steps = %d", args.gradient_accumulation_steps)
    logger.info("  Total optimization steps = %d", t_total)

    global_step = 0
    epochs_trained = 0
    steps_trained_in_current_epoch = 0
    # Check if continuing training from a checkpoint
    if os.path.exists(args.model_name_or_path):
        # set global_step to global_step of last saved checkpoint from model path
        try:
            global_step = int(args.model_name_or_path.split("-")[-1].split("/")[0])
        except ValueError:
            global_step = 0
        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)
        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)

        logger.info("  Continuing training from checkpoint, will skip to saved global_step")
        logger.info("  Continuing training from epoch %d", epochs_trained)
        logger.info("  Continuing training from global step %d", global_step)
        logger.info("  Will skip the first %d steps in the first epoch", steps_trained_in_current_epoch)

    tr_loss, logging_loss = 0.0, 0.0
    model.zero_grad()
    train_iterator = trange(
        epochs_trained, int(args.num_train_epochs), desc="Epoch", disable=args.local_rank not in [-1, 0],
    )

    set_seed(0)  # Added here for reproductibility
    for _ in train_iterator:
        epoch_iterator = tqdm(train_dataloader, desc="Iteration", disable=args.local_rank not in [-1, 0], mininterval=10)
        for step, batch in enumerate(epoch_iterator):

            # Skip past any already trained steps if resuming training
            if steps_trained_in_current_epoch > 0:
                steps_trained_in_current_epoch -= 1
                continue
#############################
            model.train()
            # batch = tuple(t.to(args.device) for t in batch)
####################################

            # inputs = {"input_ids": batch[0], "attention_mask": batch[1], "labels": batch[3]}
            # if args.model_type != "distilbert":
            #     inputs["token_type_ids"] = (
            #         batch[2] if args.model_type in ["bert", "xlnet", "albert"] else None
            #     )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids
            # outputs = model(**inputs)
            outputs = model(tokens_in_batch=batch)
#######################################
            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)

            if args.n_gpu > 1:
                loss = loss.mean()  # mean() to average on multi-gpu parallel training
            if args.gradient_accumulation_steps > 1:
                loss = loss / args.gradient_accumulation_steps

            if args.fp16:
                with amp.scale_loss(loss, optimizer) as scaled_loss:
                    scaled_loss.backward()
            else:
                loss.backward()

            tr_loss += loss.item()
            if (step + 1) % args.gradient_accumulation_steps == 0:
                if args.fp16:
                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)
                else:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                optimizer.step()
                scheduler.step()  # Update learning rate schedule
                model.zero_grad()
                global_step += 1

                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:
                    logs = {}
                    if (
                        args.local_rank == -1 and args.evaluate_during_training
                    ):  # Only evaluate when single GPU otherwise metrics may not average well
                        results = evaluate(args, model, eval_dataset = valid_dataset)
                        for key, value in results.items():
                            eval_key = "eval_{}".format(key)
                            logs[eval_key] = value
                        # results = evaluate(args, model, eval_dataset = test_dataset)
                        # for key, value in results.items():
                        #     eval_key = "test_{}".format(key)
                        #     logs[eval_key] = value

                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps
                    learning_rate_scalar = scheduler.get_lr()[0]
                    logs["learning_rate"] = learning_rate_scalar
                    logs["loss"] = loss_scalar
                    logging_loss = tr_loss

                    for key, value in logs.items():
                        tb_writer.add_scalar(key, value, global_step)
                    print(json.dumps({**logs, **{"step": global_step}}, indent=2))
                    ml_logger.append(logs)

                # if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:
                #     # Save model checkpoint
                #     output_dir = os.path.join(args.output_dir, "checkpoint-{}".format(global_step))
                #     if not os.path.exists(output_dir):
                #         os.makedirs(output_dir)
                #     model_to_save = (
                #         model.module if hasattr(model, "module") else model
                #     )  # Take care of distributed/parallel training
                #     model_to_save.save_pretrained(output_dir)
                #     tokenizer.save_pretrained(output_dir)

                #     torch.save(args, os.path.join(output_dir, "training_args.bin"))
                #     logger.info("Saving model checkpoint to %s", output_dir)

                #     torch.save(optimizer.state_dict(), os.path.join(output_dir, "optimizer.pt"))
                #     torch.save(scheduler.state_dict(), os.path.join(output_dir, "scheduler.pt"))
                #     logger.info("Saving optimizer and scheduler states to %s", output_dir)

            if args.max_steps > 0 and global_step > args.max_steps:
                epoch_iterator.close()
                break
        if args.max_steps > 0 and global_step > args.max_steps:
            train_iterator.close()
            break

    if args.local_rank in [-1, 0]:
        tb_writer.close()

    return global_step, tr_loss / global_step


def evaluate(args, model, eval_dataset):
    results = {}

    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)
    # Note that DistributedSampler samples randomly
    eval_sampler = SequentialSampler(eval_dataset)
    # eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)
    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=1, collate_fn=lambda x:x)

    # Eval!
    eval_loss = 0.0
    nb_eval_steps = 0
    preds = None
    out_label_ids = None

    for batch in tqdm(eval_dataloader, desc="Evaluating", mininterval =10):
        model.eval()

        with torch.no_grad():
            outputs = model(tokens_in_batch=batch)
            tmp_eval_loss, logits = outputs[:2]

            eval_loss += tmp_eval_loss.mean().item()
        nb_eval_steps += 1
        if preds is None:
            preds = logits.detach().cpu().numpy()
            # out_label_ids = inputs["labels"].detach().cpu().numpy()
            out_label_ids = np.array([tokens.label for tokens in batch])
        else:
            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)
            # out_label_ids = np.append(out_label_ids, inputs["labels"].detach().cpu().numpy(), axis=0)
            out_label_ids = np.append(out_label_ids, np.array([tokens.label for tokens in batch]), axis=0)

    eval_loss = eval_loss / nb_eval_steps
    ### Probabilites [0,1] ###
    probs = preds
    ### Predictions {0,1} ###
    preds = np.argmax(preds, axis=1)
    
    ### Average Precision ###
    import sklearn
    probs=np.exp(probs[:,1])/ (np.exp(probs[:,0])+np.exp(probs[:,1]))
    results["ACC:"]=sklearn.metrics.accuracy_score(out_label_ids, preds)
    results["F1:"]=sklearn.metrics.f1_score(out_label_ids, preds)
    results["AP:"]=sklearn.metrics.average_precision_score(out_label_ids, probs)
    results["AUROC:"]=sklearn.metrics.roc_auc_score(out_label_ids, probs)
    results["LOSS:"]=eval_loss
    ###
    return results


if 1==1:
    # Set seed
    set_seed(0)

    config_class, model_class, tokenizer_class = BertConfig, BertMaxPoolMafiaClassification, BertTokenizer

    config = config_class.from_pretrained(
        args.model_name_or_path,
        cache_dir=args.cache_dir if args.cache_dir else None,
    )
    tokenizer = tokenizer_class.from_pretrained(
        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,
        do_lower_case=args.do_lower_case,
        cache_dir=args.cache_dir if args.cache_dir else None,
    )
    model = model_class.from_pretrained(
        args.model_name_or_path,
        from_tf=bool(".ckpt" in args.model_name_or_path),
        config=config,
        cache_dir=args.cache_dir if args.cache_dir else None,
    )
    model.to(args.device)

    features = {
        "train":[],
        "valid":[],
        "test":[]
    }

    cached_features_file = os.path.join(
        args.data_dir,
        "cached_{}_{}_{}_{}".format(
            "dev" if evaluate else "train",
            list(filter(None, args.model_name_or_path.split("/"))).pop(),
            str(args.max_seq_length),
            str("mafia"),
        ),
    )
    if os.path.exists(cached_features_file) and not args.overwrite_cache:
        logger.info("Loading features from cached file %s", cached_features_file)
        features = torch.load(cached_features_file)
    else:
        import pandas as pd
        from transformers import InputFeatures

        for feature_k, feature_v in features.items():
          docs={
              "train":train_docs,
              "valid":valid_docs,
              "test":test_docs
          }[feature_k]
          for (ex_index, (row_i, row)) in tqdm(enumerate(docs.iterrows())):
              full_length_encoded = tokenizer.encode(row.content, add_special_tokens=False, max_length=0)
              #########
              # if ex_index > 100:
              #   break
              # if len(full_length_encoded) < 5000:
              #   continue
                ############
              

              input_ids_list = []
              attention_mask_list = []
              token_type_ids_list = []

              input_ids_list = full_length_encoded

              feature_v.append(
                  InputFeatures(
                      input_ids=input_ids_list, attention_mask=attention_mask_list, token_type_ids=token_type_ids_list, label=1 if row.scum else 0
                  )
              )

        logger.info("Saving features into cached file %s", cached_features_file)
        torch.save(features, cached_features_file)

    from torch.utils.data import Dataset
    class MyDS(Dataset):
      def __init__(self, features):
        self.features=features
      def __getitem__(self, idx):
        return self.features[idx]
      def __len__(self):
        return len(self.features)

    train_dataset = MyDS(features['train'])    
    valid_dataset = MyDS(features['valid'])
    test_dataset = MyDS(features['test'])
    
    #####################################################################################
    ####################################################################################

    # Training
    if args.do_train:
        # train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)
        global_step, tr_loss = train(args, train_dataset, model, tokenizer)
        logger.info(" global_step = %s, average loss = %s", global_step, tr_loss)
    #
    # # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()
    # if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):
    #     # Create output directory if needed
    #     if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:
    #         os.makedirs(args.output_dir)
    #
    #     logger.info("Saving model checkpoint to %s", args.output_dir)
    #     # Save a trained model, configuration and tokenizer using `save_pretrained()`.
    #     # They can then be reloaded using `from_pretrained()`
    #     model_to_save = (
    #         model.module if hasattr(model, "module") else model
    #     )  # Take care of distributed/parallel training
    #     model_to_save.save_pretrained(args.output_dir)
    #     tokenizer.save_pretrained(args.output_dir)
    #
    #     # Good practice: save your training arguments together with the trained model
    #     torch.save(args, os.path.join(args.output_dir, "training_args.bin"))
    #
    #     # Load a trained model and vocabulary that you have fine-tuned
    #     model = model_class.from_pretrained(args.output_dir)
    #     tokenizer = tokenizer_class.from_pretrained(args.output_dir)
    #     model.to(args.device)
    #
    # # Evaluation
    # results = {}
    # if args.do_eval and args.local_rank in [-1, 0]:
    #     tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)
    #     checkpoints = [args.output_dir]
    #     if args.eval_all_checkpoints:
    #         checkpoints = list(
    #             os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + "/**/" + WEIGHTS_NAME, recursive=True))
    #         )
    #         logging.getLogger("transformers.modeling_utils").setLevel(logging.WARN)  # Reduce logging
    #     logger.info("Evaluate the following checkpoints: %s", checkpoints)
    #     for checkpoint in checkpoints:
    #         global_step = checkpoint.split("-")[-1] if len(checkpoints) > 1 else ""
    #         prefix = checkpoint.split("/")[-1] if checkpoint.find("checkpoint") != -1 else ""
    #
    #         model = model_class.from_pretrained(checkpoint)
    #         model.to(args.device)
    #         result = evaluate(args, model, tokenizer, prefix=prefix)
    #         result = dict((k + "_{}".format(global_step), v) for k, v in result.items())
    #         results.update(result)
    #
    # return results
    #


# main()

evaluate(args, model, eval_dataset = test_dataset)

!cp /content/drive/My\ Drive/cached_dev_bert-base-cased-conversational_384_mafia .

!nvidia-smi

tokenizer.encode_plus("[PAD]")

tokenizer.decode("99")